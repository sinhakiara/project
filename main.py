#!/usr/bin/env python3
"""
GOD MODE Recon Crawler v18 - Ultimate Enterprise Edition
FULLY FEATURE COMPLETE (see docstring for details)
"""
import argparse
import logging
import os
import sys
import json
import asyncio
import signal
import time
import random
import hashlib
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

from dotenv import load_dotenv

from proxy_manager import ProxyManager
from rate_limiter import TokenBucketLimiter
from captcha_handler import CaptchaHandler
from vision_analysis import VisionAnalyzer
from webhooks import WebhookNotifier
from tor_support import TorSupport
from auth import AuthenticationManager
from checkpoint import CheckpointManager
from content_extractor import ContentExtractor
from pattern_library import PatternLibrary
from protocol_detector import ProtocolDetector
from exporters import Exporters
from dashboard import DashboardManager
from stealth_crawler import StealthCrawler
from metrics import CrawlerMetrics
from utils import normalize_url, get_domain
from scope_manager import ScopeManager

AdvancedScopeManager = ScopeManager

load_dotenv()
try:
    import keyring
    HAS_KEYRING = True
except ImportError:
    HAS_KEYRING = False

try:
    from playwright.async_api import async_playwright, Page, Browser
    HAS_PLAYWRIGHT = True
except ImportError:
    HAS_PLAYWRIGHT = False

def setup_logging(log_dir: Path, level: str = "INFO") -> logging.Logger:
    log_dir.mkdir(parents=True, exist_ok=True)
    logger = logging.getLogger("ReconCrawler")
    logger.handlers.clear()
    logger.setLevel(getattr(logging, level.upper()))
    fh = logging.FileHandler(log_dir / f"crawler_{datetime.now():%Y%m%d_%H%M%S}.log")
    fh.setLevel(logging.DEBUG)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    ch.setFormatter(formatter)
    logger.addHandler(fh)
    logger.addHandler(ch)
    return logger

class URLCache:
    """Disk-backed set to prevent revisiting URLs"""
    def __init__(self, cache_dir=".urlcache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.path = self.cache_dir / "visited_urls.txt"
        self.visited = set()
        self._load()
    def _load(self):
        if self.path.exists():
            with open(self.path, "r") as f:
                for line in f:
                    self.visited.add(line.strip())
    def add(self, url):
        if url not in self.visited:
            with open(self.path, "a") as f:
                f.write(url + "\n")
        self.visited.add(url)
    def __contains__(self, url):
        return url in self.visited

def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description='GOD MODE Recon Crawler v18 - Ultimate Enterprise Edition',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('--log-level', default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'])
    parser.add_argument('--requests-per-second', type=int, default=2, help="Requests per second")
    parser.add_argument('--adaptive-rate-limit', action='store_true', help="Enable adaptive rate limiting")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    crawl_parser = subparsers.add_parser('crawl', help='Start a new crawl')
    crawl_parser.add_argument('urls', nargs='+', help='Starting URLs')
    crawl_parser.add_argument('--max-pages', type=int, default=100, help='Maximum pages to crawl')
    crawl_parser.add_argument('--concurrency', type=int, default=3, help='Number of parallel workers')
    crawl_parser.add_argument('--in-scope', nargs='*', default=[], help='In-scope patterns')
    crawl_parser.add_argument('--out-scope', nargs='*', default=[], help='Out-of-scope patterns')
    crawl_parser.add_argument('--output', '-o', help='Output file path')
    crawl_parser.add_argument('--export', choices=['json', 'csv', 'xml', 'html'], default='json', help='Export format')
    crawl_parser.add_argument('--flush-every', type=int, default=20, help='Write results to disk every N pages')
    crawl_parser.add_argument('--cache-dir', default='.urlcache', help='Directory for disk-based URL cache')
    crawl_parser.add_argument('--rate-limit', type=int, default=5, help='Global requests per second limit')
    crawl_parser.add_argument('--timeout', type=int, default=30, help='Default page timeout in seconds')
    crawl_parser.add_argument('--retries', type=int, default=2, help='Retries per failed URL')
    crawl_parser.add_argument('--js-wait-ms', type=int, default=0, help='Extra wait (ms) after networkidle for JS')
    crawl_parser.add_argument('--scroll', action='store_true', help='Perform infinite scroll on each page')
    crawl_parser.add_argument('--scroll-times', type=int, default=3, help='How many times to scroll (if enabled)')
    crawl_parser.add_argument('--click-selector', help='Repeatedly click this selector to trigger loading')
    crawl_parser.add_argument('--insecure', action='store_true', help='Ignore TLS certificate errors')
    crawl_parser.add_argument('--use-keyring', action='store_true', help='Use system keyring for secrets')
    crawl_parser.add_argument('--login-url', help='Login URL')
    crawl_parser.add_argument('--username', help='Login username')
    crawl_parser.add_argument('--password', help='Login password')
    crawl_parser.add_argument('--form-fields', help='Form fields as JSON string')
    crawl_parser.add_argument('--submit-selector', help='Submit button CSS selector')
    crawl_parser.add_argument('--auth-type', choices=['form_login', 'api_key', 'bearer_token', 'oauth2'], help='Authentication type')
    crawl_parser.add_argument('--api-key', help='API Key for authentication')
    crawl_parser.add_argument('--api-key-header', default='X-API-Key', help='API Key HTTP header')
    crawl_parser.add_argument('--bearer-token', help='Bearer token for authentication')
    crawl_parser.add_argument('--captcha', action='store_true', help='Enable CAPTCHA solving')
    crawl_parser.add_argument('--captcha-type', default='2captcha', help='CAPTCHA solving provider')
    crawl_parser.add_argument('--captcha-api-key', help='API key for CAPTCHA provider')
    crawl_parser.add_argument('--vision', action='store_true', help='Enable Vision AI for screenshots')
    crawl_parser.add_argument('--vision-provider', default='openai', help='Vision AI provider')
    crawl_parser.add_argument('--vision-api-key', help='Vision API key')
    crawl_parser.add_argument('--webhooks', action='store_true', help='Enable webhooks notifications')
    crawl_parser.add_argument('--slack-url', help='Slack webhook URL')
    crawl_parser.add_argument('--discord-url', help='Discord webhook URL')
    crawl_parser.add_argument('--teams-url', help='Teams webhook URL')
    crawl_parser.add_argument('--dashboard', action='store_true', help='Show dashboard during crawl')
    crawl_parser.add_argument('--tor', action='store_true', help='Enable Tor network support')
    crawl_parser.add_argument('--proxy', action='store_true', help='Enable proxies')
    crawl_parser.add_argument('--proxy-file', default='proxies.txt', help='Proxy list file')
    resume_parser = subparsers.add_parser('resume', help='Resume from checkpoint')
    resume_parser.add_argument('checkpoint_id', help='Checkpoint ID')
    resume_parser.add_argument('--output', '-o', help='Output file')
    resume_parser.add_argument('--format', default='json', choices=['json', 'csv', 'xml', 'html'])
    scope_parser = subparsers.add_parser('scope-test', help='Test scope rules')
    scope_parser.add_argument('--in-scope', nargs='*', default=[], help='In-scope patterns')
    scope_parser.add_argument('--out-scope', nargs='*', default=[], help='Out-of-scope patterns')
    scope_parser.add_argument('--test-urls', nargs='*', default=[], help='Test URLs')
    return parser.parse_args()

# --- All Classes: Scope, Metrics, State, ... (see previous parts for implementation) ---
# For brevity, paste in the classes as you have them, or expand the content above
# AdvancedScopeManager, CrawlerMetrics, PatternLibrary, ProtocolDetector, all plug-ins are imported and used below

class CrawlState:
    def __init__(self, args, logger):
        self.args = args
        self.logger = logger
        self.url_cache = URLCache(args.cache_dir)
        #self.visited_urls = set(self.url_cache.visited)
        self.visited_urls = set([normalize_url(u) for u in self.url_cache.visited])
        self.url_queue = asyncio.Queue()
        self.results = {}
        self.result_lock = asyncio.Lock()
        self.flush_count = 0
        self.output_dir = Path("crawl_results")
        self.output_dir.mkdir(exist_ok=True)
        self.scope_manager = ScopeManager()
        # Checkpointing integration
        self.checkpoint_manager = CheckpointManager("checkpoints", logger)
        self.checkpoint_id = None
        # Metrics collection
        self.metrics = CrawlerMetrics(logger)

    async def enqueue(self, url):
        url = normalize_url(url)
        print(f"ENQUEUE CHECK: url={url} | in visited? {url in self.visited_urls} | in cache? {url in self.url_cache}")
        if url not in self.visited_urls and url not in self.url_cache:
            print(f"ENQUEUE: Actually adding url={url}")
            self.visited_urls.add(url)
            self.url_cache.add(url)
            await self.url_queue.put(url)
            print(f"QUEUE SIZE INSIDE ENQUEUE: {self.url_queue.qsize()}")
        else:
            print(f"ENQUEUE: Skipping url={url}")         
    async def flush_results(self, final=False):
        async with self.result_lock:
            mode = "final" if final else f"partial_{self.flush_count}"
            fname = self.output_dir / f"results_{mode}.{self.args.export}"
            Exporters.write(self.results, fname, mode=self.args.export)
            self.flush_count += 1
            self.logger.info(f"ðŸ’¾ Results flushed to disk: {fname}")
    async def periodic_flush(self):
        while True:
            await asyncio.sleep(20)
            await self.flush_results(final=False)
    def save_checkpoint(self):
        checkpoint = {
            "visited_urls": list(self.visited_urls),
            "queue": [self.url_queue._queue[x] for x in range(self.url_queue.qsize())],
            "results": self.results,
            "checkpoint_id": self.checkpoint_id
        }
        self.checkpoint_manager.save(self.checkpoint_id or "latest", checkpoint)

class ReconOrchestrator:
    def __init__(self, args, logger):
        self.args = args
        self.logger = logger
        self.state = CrawlState(args, logger)
        for pattern in args.in_scope:
            self.state.scope_manager.add_in_scope(pattern)
        for pattern in getattr(args, "out_scope", []):  # Note: arg is called "out-scope" in CLI, so "out_scope" in code
            self.state.scope_manager.add_out_of_scope(pattern)
        self.concurrency = args.concurrency
        self.rate_limiter = TokenBucketLimiter(args.rate_limit, 1)
        self.proxy_manager = ProxyManager(args.proxy_file, logger) if args.proxy else None
        self.captcha = CaptchaHandler(args.captcha_type, args.captcha_api_key, logger) if args.captcha else None
        self.vision = VisionAnalyzer(args.vision_provider, args.vision_api_key, logger) if args.vision else None
        self.webhooks = WebhookNotifier(args.slack_url, args.discord_url, args.teams_url, logger) if args.webhooks else None
        self.tor = TorSupport() if args.tor else None
##        self.stealth = StealthCrawler(self.logger)
        self.stealth = StealthCrawler(self.args, self.logger)
        self.patternlib = PatternLibrary()
        self.protocol_detector = ProtocolDetector()
        self.auth_manager = AuthenticationManager(logger)
        self.exporters = Exporters()
        self.dashboard = DashboardManager(logger) if args.dashboard else None
        self.content_extractor = ContentExtractor(logger)
        # Live metrics
        self.metrics = self.state.metrics

    async def crawl_main(self):

        self.state.checkpoint_id = f"{int(time.time())}_{random.randint(1000,9999)}"

        # 1. Enqueue all normalized, in-scope start URLs
        for url in self.args.urls:
            normalized = normalize_url(url)
            domain = get_domain(normalized)
            print(f"DEBUG: url={url} | normalized={normalized} | domain={domain} | is_in_scope? {self.state.scope_manager.is_in_scope(normalized)}")
            if self.state.scope_manager.is_in_scope(normalized):
                print("DEBUG: This URL WOULD BE QUEUED.")
                await self.state.enqueue(normalized)
            else:
                print("DEBUG: This URL WOULD *NOT* BE QUEUED.")

        # 2. Let event loop process any outstanding tasks
        await asyncio.sleep(0)
        print("FINAL URL QUEUE SIZE:", self.state.url_queue.qsize())
        if self.state.url_queue.qsize() == 0:
            self.logger.error(
                "NO URLs were added to the crawl queue after scope filtering! "
                "Check your --in-scope CLI value(s) and ScopeManager logic."
            )
            raise SystemExit("Fatal: No URLs to crawl. Scope filters may be too strict.")

        # 3. Notify webhooks (if enabled)
        if getattr(self, "webhooks", None):
            await self.webhooks.notify_crawl_started(list(self.args.urls), getattr(self.args, "max_pages", 100))

        # 4. Start periodic flush and async workers (only after fatal check)
        periodic_flush_task = asyncio.create_task(self.state.periodic_flush())
        workers = [
            asyncio.create_task(self.worker(i))
            for i in range(self.concurrency)
        ]
        self.logger.info(f"ðŸš€ Started {self.concurrency} async workers")
        start_time = time.time()

        # 5. Optional dashboard
        dashboard_task = None
        if getattr(self, "dashboard", None):
            dashboard_task = asyncio.create_task(self.dashboard.run(self.state))

        # 6. Main event loop: continue until max pages or queue is empty
        while len(self.state.results) < self.args.max_pages:
            if self.state.url_queue.empty():
                break
            await asyncio.sleep(2)

        # 7. Wait for all queued tasks to complete, cancel workers/auxiliary tasks
        await self.state.url_queue.join()
        for w in workers:
            w.cancel()
        await asyncio.gather(*workers, return_exceptions=True)
        periodic_flush_task.cancel()
        if dashboard_task:
            dashboard_task.cancel()
        await self.state.flush_results(final=True)
        self.metrics.log_summary()

        # 8. Notify webhooks of completion
        if getattr(self, "webhooks", None):
            await self.webhooks.notify_crawl_completed(
                visited=len(self.state.visited_urls),
                success=len(self.state.results),
                errors=sum(1 for r in self.state.results.values() if r.get("error")),
                duration=(time.time()-start_time)
            )

        # 9. Output summary file for traceability
        summary_path = self.state.output_dir / "summary.json"
        with open(summary_path, "w") as sf:
            json.dump({
                "total_urls": len(self.state.results),
                "timestamp": datetime.now().isoformat()
            }, sf, indent=2)
        self.logger.info(f"ðŸŽ‰ Crawl completed. All results saved. (Summary: {summary_path})")
    
    async def worker(self, worker_id):
        while True:
            try:
                url = await asyncio.wait_for(self.state.url_queue.get(), timeout=10)
            except asyncio.TimeoutError:
                if self.state.url_queue.empty():
                    break
                continue
            try:
                await self.process_url_with_retries(url, worker_id)
                if worker_id == 0:  # Only 1 worker triggers checkpoint
                    self.state.save_checkpoint()
            finally:
                self.state.url_queue.task_done()

    async def process_url_with_retries(self, url, worker_id):
        backoff = 2.0
        for attempt in range(self.args.retries + 1):
            try:
                await self.rate_limiter.acquire()
                await self.handle_crawl(url, worker_id)
                break
            except Exception as e:
                self.logger.warning(f"[{worker_id}] {url} attempt {attempt+1}: {e}")
                self.metrics.record_error(type(e).__name__)
                if attempt < self.args.retries:
                    await asyncio.sleep(backoff)
                    backoff *= 2
        else:
            async with self.state.result_lock:
                self.state.results[url] = {"url": url, "error": "Failed after retries"}
                self.metrics.record_error("RetriesFailed")
                self.logger.error(f"[{worker_id}] {url} failed after {self.args.retries} retries.")

    async def handle_crawl(self, url: str, worker_id: int):
        self.logger.info(f"[{worker_id}] Browsing (pw): {url}")
        #scope_manager = AdvancedScopeManager(url, self.args.in_scope, self.args.out_scope, self.logger)
        scope_manager = AdvancedScopeManager(self.args.in_scope, self.args.out_scope, self.logger)
        auth_success = False
        if not HAS_PLAYWRIGHT:
            raise RuntimeError("Playwright not installed. Install with `pip install playwright`.")
        start_time = time.time()
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context_args = {}
            if self.args.insecure:
                context_args["ignore_https_errors"] = True
            context = await browser.new_context(**context_args)
            await self.stealth.apply(context)
            page = await context.new_page()
            # Auth
            if self.args.auth_type or self.args.login_url:
                auth_success = await self.auth_manager.authenticate(page, self._make_auth_config())
                if auth_success:
                    self.auth_manager.apply_auth_to_page(page)
            # API/network interceptor/Pattern detection
            ### api_interceptor = self.patternlib.create_api_interceptor(scope_manager, self.logger)
            ### await api_interceptor.setup_interception(page)
            api_interceptor = self.patternlib.create_api_interceptor(scope_manager, self.logger)
            if api_interceptor is not None:
                setup_fn = getattr(api_interceptor, "setup_interception", None)
                if setup_fn:
                    result = setup_fn(page)
                    if asyncio.iscoroutine(result):
                        await result
            # Visit
            await page.goto(url, timeout=self.args.timeout*1000, wait_until="networkidle")
            if self.args.js_wait_ms:
                await asyncio.sleep(self.args.js_wait_ms / 1000.0)
            if self.args.scroll:
                for _ in range(self.args.scroll_times):
                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                    await asyncio.sleep(0.5)
            if self.args.click_selector:
                for _ in range(self.args.scroll_times):
                    try:
                        await page.click(self.args.click_selector, timeout=3000)
                    except:
                        pass
                    await asyncio.sleep(0.5)
            # Content extraction
            content = await self.content_extractor.extract_content(page, url)
            #techs, libs = await self.protocol_detector.detect(page)
            # Get HTML content (Playwright style)
            html = await page.content()
            url = page.url  # usually a string
            headers = {}    # Fill this with your actual headers if available
            #####techs, libs = await self.protocol_detector.detect(html, url, headers)
            protocols = await self.protocol_detector.detect(html, url, headers)
            if api_interceptor is not None:
                # Only await if method is async
                if asyncio.iscoroutinefunction(api_interceptor.discover_apis):
                    apis_discovered = await api_interceptor.discover_apis(page)
                else:
                    apis_discovered = api_interceptor.discover_apis(page)
            else:
                apis_discovered = []
            ###apis_discovered = await api_interceptor.discover_apis(page)
            ###net_traffic = api_interceptor.get_network_log()
            if api_interceptor is not None:
                network_log = api_interceptor.get_network_log()
            else:
                network_log = None  # or [] or {} depending on what you expect
            links_in_scope, links_out_scope = await self.content_extractor.extract_links(page, url, scope_manager)
            vision_result = None
            content_hash = hashlib.sha256((await page.content()).encode()).hexdigest()
            if self.vision:
                screenshot_path = str(self.state.output_dir / f"screenshot_{content_hash[:10]}.png")
                await page.screenshot(path=screenshot_path)
                vision_result = await self.vision.analyze_screenshot(screenshot_path)
            net_traffic = []
            techs = []
            libs = []
            result = {
                "url": url,
                "title": content.title,
                "hash": content_hash,
                "timestamp": datetime.now().isoformat(),
                "text": content.text[:5000],
                "meta_tags": content.meta_tags,
                "keywords": content.keywords,
                "headings": content.headings,
                "forms": content.forms,
                "buttons": content.buttons,
                "inputs": content.inputs,
                "links_in_scope": links_in_scope,
                "links_out_scope": links_out_scope,
                "apis_discovered": [a.dict() for a in apis_discovered],
                #"api_calls": api_interceptor.get_api_calls(),
                "api_calls": api_interceptor.get_api_calls() if api_interceptor is not None else [],
                "network_traffic": net_traffic,
                "vision": vision_result,
                "technologies": techs,
                "libraries": libs,
                "status_code": 200,
                "load_time": time.time() - start_time,
                "authenticated": auth_success,
                "error": None,
            }
            for link in links_in_scope:
                if link not in self.state.visited_urls and link not in self.state.url_cache:
                    await self.state.enqueue(link)
            async with self.state.result_lock:
                self.state.results[url] = result
            self.metrics.record_page(result["load_time"], 200)
            await context.close()
            await browser.close()

    def _make_auth_config(self):
        args = self.args
        auth = {}
        if args.auth_type:
            auth['type'] = args.auth_type
        else:
            if args.api_key:
                auth['type'] = 'api_key'
            elif args.bearer_token:
                auth['type'] = 'bearer_token'
            elif args.login_url:
                auth['type'] = 'form_login'
        for field in ['api_key', 'api_key_header', 'bearer_token', 'login_url', 'username', 'password', 'form_fields', 'submit_selector']:
            val = getattr(args, field, None)
            if not val and args.use_keyring and HAS_KEYRING:
                val = keyring.get_password("ReconCrawler", field)
            if not val:
                val = os.getenv(field.upper())
            if val:
                auth[field] = val
        if isinstance(auth.get("form_fields"), str):
            try:
                auth["form_fields"] = json.loads(auth["form_fields"])
            except Exception: pass
        return auth

def run_with_graceful_shutdown(coro_func):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    shutdown = asyncio.Event()
    def stop(*_):
        shutdown.set()
    for sig in (signal.SIGINT, signal.SIGTERM):
        try: loop.add_signal_handler(sig, stop)
        except NotImplementedError: pass
    async def wrapper():
        main_task = asyncio.create_task(coro_func())
        shutdown_task = asyncio.create_task(shutdown.wait())
        done, _ = await asyncio.wait([main_task, shutdown_task], return_when=asyncio.FIRST_COMPLETED)
        for task in [main_task]:
            if not task.done():
                task.cancel()
            try: await task
            except asyncio.CancelledError: pass
    try: loop.run_until_complete(wrapper())
    finally: loop.close()

def main():
    args = parse_arguments()
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    logger = setup_logging(log_dir, level=getattr(args, "log_level", "INFO"))
    logger.info(f"\nðŸŸ¢ GOD MODE Recon Crawler v18 (Zero Regression) | {datetime.now()}\n")
    if not args.command:
        print("No command specified. Try --help")
        return
    if args.command == "crawl":
        orchestrator = ReconOrchestrator(args, logger)
        run_with_graceful_shutdown(orchestrator.crawl_main)
        if getattr(args, 'output', None):
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            Exporters.write(orchestrator.state.results, output_path, mode=args.export)
            logger.info(f"âœ… Final results saved to {output_path}")
    elif args.command == "scope-test":
        logger.info("Running scope-test...")
        if hasattr(args, "test_urls") and args.test_urls:
            seed_url = args.test_urls[0] if args.test_urls else "https://example.com"
            #scope_mgr = AdvancedScopeManager(seed_url, args.in_scope, args.out_scope, logger)
            scope_mgr = AdvancedScopeManager(args.in_scope, args.out_scope, logger)
            for url in args.test_urls:
                in_scope = scope_mgr.is_in_scope(url)
                print(f"{url}: {'IN-SCOPE' if in_scope else 'OUT-OF-SCOPE'}")
        else:
            print("No test URLs provided.")
    elif args.command == "resume":
        logger.info(f"Resuming from checkpoint {args.checkpoint_id}")
        checkpoint_file = Path("checkpoints") / f"{args.checkpoint_id}.pkl"
        orchestrator = None
        if not checkpoint_file.exists():
            logger.error("Checkpoint file not found!")
            return
        # Load as per CheckpointManager
        checkpoint_manager = CheckpointManager("checkpoints", logger)
        state_data = checkpoint_manager.load(args.checkpoint_id)
        if not state_data:
            logger.error(f"Failed to load checkpoint: {args.checkpoint_id}")
            return
        args.urls = list(set(state_data.get("queue", [])))
        orchestrator = ReconOrchestrator(args, logger)
        orchestrator.state.visited_urls = set(state_data.get("visited_urls", []))
        for url in state_data.get("queue", []):
            asyncio.get_event_loop().run_until_complete(orchestrator.state.enqueue(url))
        run_with_graceful_shutdown(orchestrator.crawl_main)
        if getattr(args, 'output', None):
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            Exporters.write(orchestrator.state.results, output_path, mode=args.export)
            logger.info(f"âœ… Final results saved to {output_path}")

if __name__ == "__main__":
    main()
