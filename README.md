# ğŸ•µï¸ StealthCrawler v17

**Advanced web crawler with stealth capabilities, distributed crawling, and AI-powered analysis**

[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸš€ Features

### Core Capabilities
- **ğŸ­ Stealth Mode**: Browser fingerprint randomization, anti-detection measures
- **âš¡ High Performance**: Async/await architecture with concurrent workers
- **ğŸ¯ Advanced Scope Management**: Wildcard patterns (`*.domain.com`, `**.domain.com`) with exclusion priority
- **ğŸŒ Distributed Crawling**: Redis-based coordination for multi-worker deployments
- **ğŸ“Š Multiple Export Formats**: JSON, CSV, XML, HTML reports
- **ğŸ”„ Resume Capability**: Checkpoint system to resume interrupted crawls

### Advanced Features
- **ğŸ¤– AI Vision Analysis**: OpenAI GPT-4 Vision and Anthropic Claude integration
- **ğŸ” Multi-Strategy Authentication**: Basic, OAuth2, form-based login
- **ğŸ§© CAPTCHA Solving**: 2Captcha and Anti-Captcha integration
- **ğŸ”€ Proxy Management**: Rotation with health checking
- **ğŸ§… Tor Support**: Anonymous crawling via Tor network
- **ğŸ“¡ Protocol Detection**: WebSocket, GraphQL, SSE, REST API detection
- **ğŸ¨ Rich Dashboard**: Real-time terminal UI with live statistics
- **ğŸ“¨ Webhook Notifications**: Slack, Discord, Microsoft Teams integration
- **ğŸ” Pattern Learning**: Self-learning URL pattern detection

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Scope Management](#scope-management)
- [Configuration](#configuration)
- [Distributed Crawling](#distributed-crawling)
- [Docker Deployment](#docker-deployment)
- [API Server](#api-server)
- [Testing](#testing)
- [Examples](#examples)
- [Architecture](#architecture)
- [Contributing](#contributing)

## ğŸ”§ Installation

### Prerequisites
- Python 3.11 or higher
- pip package manager

### Standard Installation

```bash
# Clone the repository
git clone https://github.com/Sattyam-Kasar/stealth-crawler.git
cd stealth-crawler

# Install dependencies
pip install -r requirements.txt

# Install Playwright browsers
playwright install chromium

# Or use the quick start script
chmod +x run.sh
./run.sh
```

### Docker Installation

```bash
# Build Docker image
docker build -t stealth-crawler:v17 .

# Or use Docker Compose
docker-compose up -d
```

## âš¡ Quick Start

### Basic Crawl

```bash
# Crawl a single domain
python main.py crawl https://example.com

# Crawl with depth limit
python main.py crawl https://example.com --depth 3

# Export results
python main.py crawl https://example.com --output results.json --format json
```

### With Scope Management

```bash
# Include all subdomains, exclude admin
python main.py crawl https://example.com \
  --in-scope "*.example.com" \
  --out-of-scope "admin.example.com" \
  --depth 5
```

### Test Scope Configuration

```bash
# Test your scope rules
python main.py scope-test \
  --in-scope "*.example.com" "*.test.com" \
  --out-of-scope "admin.example.com" "private.test.com" \
  --test-urls "https://api.example.com" "https://admin.example.com"
```

## ğŸ“– Usage

### Command Line Interface

StealthCrawler provides several commands:

#### 1. Crawl Command

```bash
python main.py crawl [OPTIONS] URLS...

Options:
  --depth INTEGER          Maximum crawl depth (default: 5)
  --in-scope TEXT         In-scope domain patterns (can specify multiple)
  --out-of-scope TEXT     Out-of-scope domain patterns (can specify multiple)
  --output PATH           Output file path
  --format [json|csv|xml|html]  Output format (default: json)
  --log-level [DEBUG|INFO|WARNING|ERROR]  Logging level
```

Example:
```bash
python main.py crawl https://example.com https://test.com \
  --depth 3 \
  --in-scope "*.example.com" "*.test.com" \
  --out-of-scope "admin.example.com" \
  --output crawl_results.json \
  --format json
```

#### 2. Distributed Command

```bash
# Start master node
python main.py distributed --master \
  https://example.com \
  --depth 5

# Start worker nodes
python main.py distributed --worker-id worker-1
python main.py distributed --worker-id worker-2
```

#### 3. API Server Command

```bash
python main.py server --host 0.0.0.0 --port 8000
```

#### 4. Resume Command

```bash
python main.py resume checkpoint-name \
  --output resumed_results.json
```

#### 5. Scope Test Command

```bash
python main.py scope-test \
  --in-scope "*.example.com" \
  --out-of-scope "admin.example.com" \
  --test-urls "https://api.example.com" "https://admin.example.com"
```

## ğŸ¯ Scope Management

### Overview

The scope manager is a **CRITICAL** component that controls which URLs are crawled. It supports:

- âœ… Exact domain matching
- âœ… Wildcard subdomain matching (`*.domain.com`)
- âœ… Nested wildcard matching (`**.domain.com`)
- âœ… **EXCLUSION PRIORITY**: Exclusions always override inclusions

### Wildcard Patterns

#### Single-Level Wildcard (`*.domain.com`)

Matches exactly ONE level of subdomain:

```python
# Pattern: *.example.com
âœ“ api.example.com       # Matches
âœ“ admin.example.com     # Matches
âœ— example.com           # No match (base domain)
âœ— api.v1.example.com    # No match (two levels)
```

#### Multi-Level Wildcard (`**.domain.com`)

Matches ANY number of subdomain levels:

```python
# Pattern: **.example.com
âœ“ api.example.com           # Matches
âœ“ api.v1.example.com        # Matches
âœ“ test.api.v1.example.com   # Matches
âœ— example.com               # No match (base domain)
```

### Exclusion Priority (CRITICAL!)

**Exclusions ALWAYS take precedence over inclusions:**

```bash
# Include all subdomains
--in-scope "*.example.com"

# Exclude admin subdomain
--out-of-scope "admin.example.com"

# Results:
# âœ“ api.example.com     â†’ IN SCOPE
# âœ“ test.example.com    â†’ IN SCOPE
# âœ— admin.example.com   â†’ OUT OF SCOPE (excluded!)
```

### Programmatic Usage

```python
from scope_manager import create_scope_manager

# Create scope manager
scope = create_scope_manager(
    in_scope=['*.example.com', 'test.com'],
    out_of_scope=['admin.example.com', 'private.example.com']
)

# Check if URL is in scope
if scope.is_in_scope('https://api.example.com'):
    print("URL is in scope!")

# Test URL with details
result = scope.test_url('https://admin.example.com')
print(result)
# {
#   'url': 'https://admin.example.com',
#   'domain': 'admin.example.com',
#   'in_scope': False,
#   'reason': 'EXCLUDED',
#   'matches_out_of_scope': ['exact: admin.example.com']
# }
```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file (copy from `.env.example`):

```bash
# General Settings
LOG_LEVEL=INFO
MAX_WORKERS=10
MAX_DEPTH=5

# Stealth Settings
HEADLESS=true
USER_AGENT_ROTATION=true
FINGERPRINT_RANDOMIZATION=true

# Rate Limiting
REQUESTS_PER_SECOND=2
ADAPTIVE_RATE_LIMIT=true

# Proxy Settings
USE_PROXY=false
PROXY_LIST_FILE=proxies.txt

# AI Vision
OPENAI_API_KEY=your-key-here
VISION_ANALYSIS_ENABLED=false

# Distributed Crawling
REDIS_HOST=localhost
REDIS_PORT=6379
DISTRIBUTED_MODE=false

# Webhooks
SLACK_WEBHOOK_URL=https://hooks.slack.com/...
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/...
```

### Programmatic Configuration

```python
from config import CrawlerConfig

config = CrawlerConfig()
config.max_workers = 20
config.max_depth = 10
config.headless = False
config.requests_per_second = 5

crawler = StealthCrawler(config)
```

## ğŸŒ Distributed Crawling

### Architecture

StealthCrawler uses Redis for distributed coordination:

- **Master Node**: Initializes the queue with start URLs
- **Worker Nodes**: Process URLs from the shared queue
- **Redis**: Coordinates work and stores results

### Setup

1. **Start Redis**:
```bash
docker run -d -p 6379:6379 redis:7-alpine
```

2. **Configure Workers**:
```bash
# .env
DISTRIBUTED_MODE=true
REDIS_HOST=localhost
REDIS_PORT=6379
```

3. **Run Master**:
```bash
python main.py distributed --master \
  https://example.com \
  --depth 5
```

4. **Run Workers** (in separate terminals/machines):
```bash
python main.py distributed --worker-id worker-1
python main.py distributed --worker-id worker-2
python main.py distributed --worker-id worker-3
```

### Docker Compose

```bash
# Start full distributed stack
docker-compose up -d

# Scale workers
docker-compose up -d --scale crawler-worker=5
```

## ğŸ³ Docker Deployment

### Build Image

```bash
docker build -t stealth-crawler:v17 .
```

### Run Container

```bash
docker run -it stealth-crawler:v17 \
  python main.py crawl https://example.com
```

### Docker Compose

```bash
# Start all services
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

Services included:
- `crawler-api`: REST API server
- `crawler-worker-1/2`: Distributed workers
- `redis`: Coordination
- `elasticsearch`: Data storage
- `tor`: Anonymous crawling

## ğŸ”Œ API Server

### Start Server

```bash
python main.py server --host 0.0.0.0 --port 8000
```

### API Endpoints

#### Start Crawl

```bash
curl -X POST http://localhost:8000/crawl/start \
  -H "Content-Type: application/json" \
  -d '{
    "start_urls": ["https://example.com"],
    "max_depth": 5,
    "in_scope": ["*.example.com"],
    "out_of_scope": ["admin.example.com"]
  }'
```

#### Get Status

```bash
curl http://localhost:8000/crawl/{crawl_id}/status
```

#### Get Results

```bash
curl http://localhost:8000/crawl/{crawl_id}/results?limit=100
```

#### List Crawls

```bash
curl http://localhost:8000/crawl/list
```

## ğŸ§ª Testing

### Run All Tests

```bash
# Using pytest
pytest tests/ -v

# Using make
make test
```

### Run with Coverage

```bash
pytest tests/ -v --cov=. --cov-report=html --cov-report=term
```

### Run Specific Tests

```bash
# Test scope manager only
pytest tests/test_scope_manager.py -v

# Test crawler only
pytest tests/test_crawler.py -v
```

### Linting

```bash
make lint
```

## ğŸ“š Examples

### Example 1: Basic E-commerce Crawl

```python
import asyncio
from stealth_crawler import StealthCrawler
from scope_manager import create_scope_manager
from config import CrawlerConfig

async def main():
    config = CrawlerConfig()
    config.max_depth = 3
    
    crawler = StealthCrawler(config)
    crawler.scope_manager = create_scope_manager(
        in_scope=['shop.example.com'],
        out_of_scope=['shop.example.com/admin']
    )
    
    await crawler.initialize()
    results = await crawler.crawl(['https://shop.example.com'])
    await crawler.close()
    
    print(f"Crawled {len(results)} pages")

asyncio.run(main())
```

### Example 2: Multi-Domain with Exclusions

```bash
python main.py crawl \
  https://example.com \
  https://test.com \
  --in-scope "*.example.com" "*.test.com" \
  --out-of-scope "admin.example.com" "private.test.com" "*.internal.example.com" \
  --depth 5 \
  --output multi_domain.json
```

### Example 3: Distributed Crawl with Webhooks

```bash
# .env
DISTRIBUTED_MODE=true
WEBHOOK_ENABLED=true
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Start crawl
python main.py distributed --master \
  https://example.com \
  --depth 10
```

## ğŸ—ï¸ Architecture

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  StealthCrawler v17                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Scope Managerâ”‚  â”‚ Rate Limiter â”‚  â”‚ Stealth  â”‚ â”‚
â”‚  â”‚   (Critical) â”‚  â”‚  (Adaptive)  â”‚  â”‚  Engine  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Crawler    â”‚  â”‚  Checkpoint  â”‚  â”‚  Export  â”‚ â”‚
â”‚  â”‚   Workers    â”‚  â”‚   Manager    â”‚  â”‚  Formats â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Distributed  â”‚  â”‚     API      â”‚  â”‚ Webhooks â”‚ â”‚
â”‚  â”‚   (Redis)    â”‚  â”‚   Server     â”‚  â”‚  Notify  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### File Structure

```
stealth-crawler/
â”œâ”€â”€ main.py                 # CLI entry point
â”œâ”€â”€ stealth_crawler.py      # Core crawler
â”œâ”€â”€ scope_manager.py        # CRITICAL: Scope management
â”œâ”€â”€ config.py               # Configuration
â”œâ”€â”€ utils.py                # Utilities
â”œâ”€â”€ rate_limiter.py         # Rate limiting
â”œâ”€â”€ fingerprint.py          # Fingerprint randomization
â”œâ”€â”€ auth.py                 # Authentication
â”œâ”€â”€ captcha_handler.py      # CAPTCHA solving
â”œâ”€â”€ proxy_manager.py        # Proxy rotation
â”œâ”€â”€ tor_support.py          # Tor integration
â”œâ”€â”€ vision_analysis.py      # AI vision
â”œâ”€â”€ checkpoint.py           # Checkpointing
â”œâ”€â”€ distributed.py          # Distributed crawling
â”œâ”€â”€ api_server.py           # REST API
â”œâ”€â”€ dashboard.py            # Terminal UI
â”œâ”€â”€ webhooks.py             # Notifications
â”œâ”€â”€ exporters.py            # Export formats
â”œâ”€â”€ pattern_library.py      # Pattern learning
â”œâ”€â”€ protocol_detector.py    # Protocol detection
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ Dockerfile              # Docker build
â”œâ”€â”€ docker-compose.yml      # Docker stack
â”œâ”€â”€ Makefile               # Build commands
â”œâ”€â”€ run.sh                 # Quick start
â””â”€â”€ tests/                 # Test suite
    â”œâ”€â”€ conftest.py
    â”œâ”€â”€ test_crawler.py
    â””â”€â”€ test_scope_manager.py
```

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Run the test suite
6. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- Built with [Playwright](https://playwright.dev/)
- Uses [Rich](https://rich.readthedocs.io/) for terminal UI
- Distributed crawling with [Redis](https://redis.io/)
- AI powered by [OpenAI](https://openai.com/) and [Anthropic](https://anthropic.com/)

## ğŸ“ Support

- ğŸ“§ Email: support@stealthcrawler.dev
- ğŸ’¬ Discord: https://discord.gg/stealthcrawler
- ğŸ› Issues: https://github.com/Sattyam-Kasar/stealth-crawler/issues

---

**Made with â¤ï¸ by the StealthCrawler Team**
